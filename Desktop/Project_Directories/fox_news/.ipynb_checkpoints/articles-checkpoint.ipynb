{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4469486-bf4d-4b20-a5fe-cc76e02a7fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "# for the lyrics scrape section\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51ce382e-bfca-4ff5-aa34-700351fe3f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_url = \"https://www.foxnews.com/politics\"\n",
    "news_pages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17d4718d-a28d-4ab0-93de-fa0820310b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the main page\n",
    "response = requests.get(articles_url)\n",
    "time.sleep(5 + 10*random.random())\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cdd222c-1988-4dbe-b04d-56d2212a1561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract article links \n",
    "for link in soup.find_all('a', href=True):\n",
    "    href = link['href']\n",
    "    if href.startswith('/politics') and href not in news_pages:\n",
    "        news_pages.append(href)\n",
    "\n",
    "def generate_filename_from_link(link):\n",
    "    name = link.replace('/politics/', '').replace('/', '_')\n",
    "    name += \".txt\"\n",
    "    return name\n",
    "\n",
    "if os.path.isdir(\"fox_articles\"): \n",
    "    shutil.rmtree(\"fox_articles/\")\n",
    "os.mkdir(\"fox_articles\")\n",
    "\n",
    "url_stub = \"https://www.foxnews.com\"\n",
    "total_pages = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3f31219-945f-46ae-adf2-fd1e3d918e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in news_pages: \n",
    "    if total_pages >= 25:\n",
    "        break\n",
    "\n",
    "    full_link = url_stub + link\n",
    "    response = requests.get(full_link)\n",
    "    time.sleep(5 + 10*random.random())\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extracting title and article text (adjust these selectors)\n",
    "    title = soup.find('h1').get_text(strip=True) if soup.find('h1') else 'No Title'\n",
    "    article_body = soup.find('div', class_='article-body')  # or whatever class contains the article\n",
    "    article_text = article_body.get_text(strip=True) if article_body else 'No Content'\n",
    "\n",
    "    filename = generate_filename_from_link(link)\n",
    "    file_path = os.path.join(\"fox_articles\", filename)\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(title + '\\n\\n' + article_text)\n",
    "\n",
    "    total_pages += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d2ae4e-2b82-4cfc-9169-3c49d3f8a98d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
